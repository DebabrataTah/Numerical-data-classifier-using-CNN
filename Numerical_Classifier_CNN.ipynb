{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from sklearn.model_selection import KFold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = 'cuda:0'\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "    return device\n",
    "device = get_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automobile = pd.read_csv('datasets/automobile/automobile.dat', header = None, skiprows=30, na_values=['?']) #keel\n",
    "\n",
    "dermatology = pd.read_csv('datasets/dermatology/dermatology.dat', skiprows=39, header = None, na_values=['?'])\n",
    "\n",
    "diabetes = pd.read_csv('datasets/diabetes(pima)/pima.dat', header=None, skiprows=13)\n",
    "\n",
    "ecoli = pd.read_csv('datasets/ecoli/ecoli.data', header=None, delim_whitespace=True)\n",
    "ecoli.drop(ecoli.columns[0], axis=1, inplace=True) # drooping the seq. numbers\n",
    "\n",
    "flare = pd.read_csv('datasets/flare/flare.dat', header=None, skiprows=16)\n",
    "\n",
    "iris = pd.read_csv('datasets/iris/iris.dat', header = None, skiprows=9)\n",
    "\n",
    "led7digit = pd.read_csv('datasets/led7digit/led7digit.dat', skiprows=12, header=None)\n",
    "\n",
    "monk = pd.read_csv('datasets/monk/monk-2.dat', header=None, skiprows=11)\n",
    "\n",
    "new_thyroid = pd.read_csv('datasets/new thyroid/newthyroid.dat', header=None, skiprows=10)\n",
    "\n",
    "sonar = pd.read_csv('datasets/sonar/sonar.dat', skiprows=65, header=None)\n",
    "\n",
    "vowel = pd.read_csv('datasets/vowel/vowel.dat', skiprows=18, header=None)\n",
    "\n",
    "wine = pd.read_csv('datasets/wine/wine.data', header=None)\n",
    "cols = list(wine) # Class column needs to be the last column. Not first.\n",
    "cols[0], cols[-1] = cols[-1], cols[0] # swapping 1st and last elements of column list\n",
    "wine = wine.loc[:, cols] # Assigning changed column list\n",
    "\n",
    "wisconsin = pd.read_csv('datasets/wisconsin/wisconsin.dat', skiprows=14, header=None, na_values=[' <null>'])\n",
    "\n",
    "yeast = pd.read_csv('datasets/yeast/yeast.dat', header=None, skiprows=13)\n",
    "\n",
    "zoo = pd.read_csv('datasets/zoo/zoo.dat', header=None, skiprows=21) #keel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the dataset to be selected\n",
    "\n",
    "#df = automobile.copy() \n",
    "#df = dermatology.copy() \n",
    "#df = diabetes.copy() \n",
    "#df = ecoli.copy() \n",
    "#df = flare.copy() \n",
    "#df = iris.copy()\n",
    "#df = led7digit.copy() \n",
    "#df = monk.copy()\n",
    "df = new_thyroid.copy() \n",
    "#df = sonar.copy() \n",
    "#df = vowel.copy() \n",
    "#df = wine.copy() \n",
    "#df = wisconsin.copy() \n",
    "#df = yeast.copy()\n",
    "#df = zoo.copy()\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df = df.iloc[:, :-1] \n",
    "class_df = df.iloc[:, -1] # Process feature data separately and conatinate the class column with it at the end (to prevent one hot encoding of the class column \n",
    "\n",
    "try:\n",
    "    one_hot_cats_df = pd.get_dummies(feature_df.select_dtypes(['object'])) # one_hot_cats_df contains the one hot encoded categorical features. If a categorical feature is nan, all one hot codes of it will contain 0. We need to turn this 0 to nans to represent missing values.\n",
    "    processed_df = pd.concat([feature_df, one_hot_cats_df], axis=1) # processed_df contains former features along with the one hot encode features. The formar categorical feature values are used to search for nan values. If a nan is found, the row of it's corresponding one hot encoded columns will be assigned nans.\n",
    "    print('%d categorical features present.' %(len(feature_df.select_dtypes(['object']).columns)))\n",
    "except:\n",
    "    processed_df = feature_df\n",
    "    print('No Categorical Feature present.')\n",
    "\n",
    "#processed_df.head()\n",
    "\n",
    "for i in processed_df.columns:\n",
    "    if processed_df[i].dtypes == 'object': # Former unencoded categorical feature found\n",
    "        processed_df.loc[processed_df[i].isnull(), processed_df.columns.str.startswith(str(i) + '_', na=False)] = np.nan # The formar categorical feature values are used to search for nan values. If a nan is found, the row of it's corresponding one hot encoded columns will be assigned nans.\n",
    "        \n",
    "processed_df.drop(processed_df.select_dtypes(['object']), axis=1, inplace=True) # Drop the former unencoded feature values\n",
    "\n",
    "final_df = pd.concat([processed_df, class_df], axis=1) # Add the class column to the processed feature dataframe\n",
    "final_df.iloc[:, -1] = final_df.iloc[:, -1].astype('category').cat.codes # Encoding the class column so that the classes start from 0  \n",
    "df = final_df\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape:', df.shape)\n",
    "print('\\nMissing values:', df.isnull().sum().sum())\n",
    "print('\\nClass distribution:\\n' +str( df.iloc[:, -1].value_counts()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(torch.nn.Module):   \n",
    "    \n",
    "    def __init__(self, number_of_features, number_of_classes):      \n",
    "        self.number_of_features = number_of_features\n",
    "        self.number_of_classes = number_of_classes\n",
    "        \n",
    "        self.number_of_dummy_neurons = 0\n",
    "        self.missing_value_count = 0\n",
    "        \n",
    "        self.conv1_out_channels = 64\n",
    "        self.conv3_out_channels = 128\n",
    "        \n",
    "        kernel_size = 3\n",
    "        padding = (kernel_size - 1) // 2\n",
    "                       \n",
    "        super(Network, self).__init__()\n",
    "        \n",
    "        self.bn1 = torch.nn.BatchNorm1d(1)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(self.conv1_out_channels)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(self.conv3_out_channels)\n",
    "        self.dp = torch.nn.Dropout(0.4)\n",
    "        \n",
    "        self.conv1 = torch.nn.Conv1d(in_channels=1, out_channels=self.conv1_out_channels, kernel_size=kernel_size, padding=padding)\n",
    "        self.conv2 = torch.nn.Conv1d(in_channels=self.conv1_out_channels, out_channels=self.conv1_out_channels, kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv3 = torch.nn.Conv1d(in_channels=self.conv1_out_channels, out_channels=self.conv3_out_channels, kernel_size=kernel_size, padding=padding)\n",
    "        self.conv4 = torch.nn.Conv1d(in_channels=self.conv3_out_channels, out_channels=self.conv3_out_channels, kernel_size=2, stride=2)\n",
    "        \n",
    "        self.fc1 = torch.nn.Linear(in_features=self.conv3_out_channels * (self.number_of_features // 4), out_features=self.number_of_classes) \n",
    "      \n",
    "    def forward(self, t):       \n",
    "        '''\n",
    "        When thare's missing value in the input data point vector:\n",
    "            1. Delete the input neurons coressponding to each missing value.\n",
    "            2. Apply conv1.\n",
    "            3. Apply conv2 (MaxPool).\n",
    "            4. Flatten the output of previous layer.\n",
    "            5. Now, we have to apply fc1 on the flattened output. \n",
    "               But the flattened output will be smaller in size than it would have been in case of no missing value.\n",
    "               So, the input neurons to fc1 become reduced.\n",
    "               But the in_feature argument of fc1 is fixed.\n",
    "            6. So, add dummy neurons to make input neurons of fc1 equal to the defined in_feature argument.\n",
    "            7. The weights associated with these dummy neurons must not be updated.\n",
    "            8. So, after backpropagating the loss (loss.backward())\n",
    "               Assign 0 to the grads of the weights associated with the dummy neurons.\n",
    "            9. Now, update the weights (optimizer.step()).\n",
    "        '''\n",
    "        \n",
    "        self.number_of_dummy_neurons = 0 # Needs to be reassined to 0 to prevent it from retaining it's former value.\n",
    "        self.missing_value_count = torch.isnan(t).sum().item() # number of nan values in the input vector.\n",
    "        vector_size = self.number_of_features - self.missing_value_count # vector_size = number of input neurons\n",
    "        \n",
    "        if self.missing_value_count > 0: # Missing value present in the data point\n",
    "            t = t[~torch.isnan(t)].reshape(1, 1, vector_size) # deleting nan values from the input vector\n",
    "        \n",
    "        t = self.conv1(t) \n",
    "        t = F.relu(t)\n",
    "        t = self.conv2(t) \n",
    "        t = F.relu(t)\n",
    "\n",
    "        t = self.conv3(t) \n",
    "        t = F.relu(t)\n",
    "        t = self.conv4(t) \n",
    "        t = F.relu(t)\n",
    "\n",
    "        t = t.reshape(-1, self.conv3_out_channels * (vector_size // 4)) \n",
    "        \n",
    "        t = self.dp(t)\n",
    "        if self.missing_value_count > 0: # Missing value is there. Handle the fc1 layer.\n",
    "            self.number_of_dummy_neurons = self.conv3_out_channels * (self.number_of_features // 4) - t.shape[1]\n",
    "            left_padding, right_padding = 0, self.number_of_dummy_neurons\n",
    "            new_t = F.pad(t, (left_padding, right_padding), value=0) # The number of in_features in Network.fc1 is fixied i.e. in_features=12 * (self.number_of_features // 4). So, if we deleted some input neurons flattened t will have to be padded with random values to match the defined in_features number \n",
    "            new_t = self.fc1(new_t)\n",
    "            return new_t   \n",
    "        t = self.fc1(t)\n",
    "        return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumericalDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, x, y):\n",
    "        '''\n",
    "        x: the attributes\n",
    "        y: the class labels\n",
    "        '''\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self): \n",
    "        return (len(self.x))\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        x = self.x[i, :] # Returns a vector\n",
    "        x = x.reshape(1, x.shape[0]) # Adding an extra dimension for channel [shape: (1, no_of_attributes)]\n",
    "        x = x.double() # To prevent errors like -> Expected object of scalar type Double but got scalar type Float \n",
    "   \n",
    "        y = self.y[i].item() # Assuming y is a single number denoting the class\n",
    "        y = int(y) # Assuming the class is denoted by integer\n",
    "        return (x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tenfcv(df, total_epochs=50):\n",
    "    np_data = df.values # fetching data from dataset\n",
    "    np.random.shuffle(np_data)\n",
    "\n",
    "    number_of_classes = int(max(np_data[:, -1])) + 1 \n",
    "    number_of_attributes = np_data.shape[1] - 1 # negate one as we dont consider class as attribute\n",
    "\n",
    "    t_data = torch.tensor(np_data)\n",
    "    x = t_data[:, :-1] # Attributes\n",
    "    y = t_data[:, -1] # Classes\n",
    "    dataset = NumericalDataset(x, y) # creating the Dataset object\n",
    "\n",
    "    kf = KFold(n_splits = 10)\n",
    "    total_accuracy = 0\n",
    "\n",
    "    fold = 1\n",
    "    for train_index, test_index in kf.split(dataset): # each fold\n",
    "        train_set = torch.utils.data.Subset(dataset, train_index) \n",
    "        test_set = torch.utils.data.Subset(dataset, test_index) \n",
    "\n",
    "        batch_size = 1 \n",
    "        train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size) \n",
    "        test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size)\n",
    "\n",
    "        net = Network(number_of_attributes, number_of_classes).double().to(device) # a new object is created for each fold\n",
    "        net = net.double()\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=0.001, eps=1e-5, weight_decay=1e-9) \n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        for epoch in range(total_epochs): \n",
    "            net = net.train()\n",
    "            for batch in train_loader: # each Batch [0 to len(train_subset)/batch_size]\n",
    "                x, y = batch\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "\n",
    "                preds = net(x) # Pass Batch (Forward pass)\n",
    "                loss = criterion(preds, y) # Calculate loss\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward() # Calculate Gradients\n",
    "                if net.missing_value_count > 0: # Missing value present in the data point.\n",
    "                    fc1_weight, fc1_bias = net.fc1.parameters() # Get the weights of the 1st fully connected layer and make the grads of the weights associated with the dummy neurons to 0.\n",
    "                    fc1_weight.grad[:, -net.number_of_dummy_neurons:] = 0 # Select each row's last cnn_net.number_of_dummy_neurons number of elements.\n",
    "                optimizer.step() #Update Weights\n",
    "\n",
    "        #testing\n",
    "        net = net.eval()\n",
    "        correct_predictions = 0\n",
    "        for batch in test_loader:\n",
    "            test_x, test_y = batch\n",
    "            test_x = test_x.to(device)\n",
    "            test_y = test_y.to(device)\n",
    "\n",
    "            preds = net(test_x)\n",
    "            loss = F.cross_entropy(preds, test_y)\n",
    "            \n",
    "            correct_predictions += preds.argmax(dim=1).eq(test_y).sum().item()\n",
    "\n",
    "        accuracy = correct_predictions / len(test_set) * 100\n",
    "        print('\\nFold: ', fold, '/', '10 loss:', loss.item())\n",
    "        print('Correct predictions: ', correct_predictions, '/', len(test_set))\n",
    "        print('Accuracy:', accuracy)\n",
    "        print()\n",
    "        total_accuracy += accuracy\n",
    "        fold += 1\n",
    "\n",
    "    print()\n",
    "    print('10CV result:')\n",
    "    final_accuracy = total_accuracy / 10\n",
    "    print('Accuracy:', final_accuracy)\n",
    "    return final_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_accuracy = 0\n",
    "for i in range(10):\n",
    "    print('\\n----------------------------------------------\\n')\n",
    "    print('%dth 10CV:' %(i+1))\n",
    "    total_accuracy += tenfcv(df, total_epochs=1)\n",
    "\n",
    "print('\\n----------------------------------------------')\n",
    "print('\\n----------------------------------------------')\n",
    "print('Final Result:')\n",
    "print('Average accuracy after doing 10cv 10 times:', total_accuracy / 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
